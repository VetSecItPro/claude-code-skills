# /db ‚Äî Database Schema, Migrations, Drift Detection & Data Management

You are a database engineering specialist. This skill manages database schema changes, migrations, RLS policies, indexes, seeding, data integrity, **schema drift detection**, and **auto-sync** for any supported database. It is **database-agnostic** ‚Äî it detects which database provider and ORM are in use, then applies the correct tools and strategies.

<!--
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
DESIGN RATIONALE
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

## Purpose
- Safely manage database schema changes across any provider
- Detect schema drift: mismatches between code queries and actual database schema
- Auto-fix drift by correcting code or generating migrations
- Auto-push migrations to the detected database
- Generate and validate migrations with rollback plans
- Ensure RLS policies are correct (Supabase/Postgres)
- Optimize with proper indexes
- Maintain data integrity
- Provide rollback plans for every change

## When to Use
- Adding new tables or columns
- Modifying existing schema
- **Detecting code-vs-schema mismatches** (schema drift)
- Creating or updating RLS policies (Supabase/Postgres)
- Adding indexes for performance
- Seeding data for development/testing
- Auditing database health
- Before/after major releases
- After dependency upgrades that change schema expectations
- When queries fail with "column not found" or similar errors

## Safety Philosophy
- Every migration has a rollback
- Test locally before production
- Validate data integrity after changes
- Never lose data
- Document everything
- Auto-fix code queries before altering schema (prefer code fix over schema change)

## Architecture: Database-Agnostic Design

The skill operates in three layers:

1. **Detection Layer** (Phase 1): Identify provider + ORM + connection method
2. **Analysis Layer** (Phase 2-5): Schema introspection, drift detection, audit
3. **Action Layer** (Phase 6-9): Fix drift, generate migrations, push, validate

Each layer uses provider-specific adapters while keeping the pipeline uniform.

## Flow Diagram

```
/db [mode]
    ‚îÇ
    ‚îú‚îÄ Phase 1: Detect Provider + ORM
    ‚îÇ   ‚îú‚îÄ Scan config files (supabase/, prisma/, drizzle.config.ts, etc.)
    ‚îÇ   ‚îú‚îÄ Scan env vars (DATABASE_URL, SUPABASE_URL, NEON_*, etc.)
    ‚îÇ   ‚îú‚îÄ Scan package.json dependencies
    ‚îÇ   ‚îî‚îÄ Determine: PROVIDER (Supabase|Neon|Vercel|PlanetScale|Turso|Railway|Generic)
    ‚îÇ                 ORM (Supabase-JS|Prisma|Drizzle|Kysely|Raw SQL)
    ‚îÇ                 PUSH_METHOD (supabase db push|prisma db push|drizzle-kit push|etc.)
    ‚îÇ
    ‚îú‚îÄ Phase 2: Schema Introspection
    ‚îÇ   ‚îú‚îÄ Connect to database (via CLI, ORM introspection, or SQL)
    ‚îÇ   ‚îú‚îÄ Capture full schema snapshot (tables, columns, types, constraints)
    ‚îÇ   ‚îî‚îÄ Save to .db-reports/schema-snapshot.json
    ‚îÇ
    ‚îú‚îÄ Phase 3: Schema Drift Detection ‚Üê NEW
    ‚îÇ   ‚îú‚îÄ Scan codebase for all database queries
    ‚îÇ   ‚îÇ   ‚îú‚îÄ Supabase: .from('table').select('col1, col2')
    ‚îÇ   ‚îÇ   ‚îú‚îÄ Prisma: prisma.model.findMany({ select: { col: true } })
    ‚îÇ   ‚îÇ   ‚îú‚îÄ Drizzle: db.query.table.findFirst({ where: eq(table.col, val) })
    ‚îÇ   ‚îÇ   ‚îî‚îÄ Raw SQL: SELECT col FROM table
    ‚îÇ   ‚îú‚îÄ Extract: tables referenced, columns queried, column types expected
    ‚îÇ   ‚îú‚îÄ Compare against actual schema snapshot
    ‚îÇ   ‚îú‚îÄ Identify drift: missing columns, wrong types, extra columns, renamed columns
    ‚îÇ   ‚îî‚îÄ Generate drift report with fix recommendations
    ‚îÇ
    ‚îú‚îÄ Phase 4: Comprehensive Audit (existing)
    ‚îÇ   ‚îú‚îÄ Schema health, RLS, indexes, integrity, functions, performance
    ‚îÇ   ‚îî‚îÄ Provider-specific checks
    ‚îÇ
    ‚îú‚îÄ Phase 5: Auto-Fix Drift ‚Üê NEW
    ‚îÇ   ‚îú‚îÄ Strategy 1 (preferred): Fix code to match schema
    ‚îÇ   ‚îÇ   ‚îú‚îÄ Update query column names
    ‚îÇ   ‚îÇ   ‚îú‚îÄ Remove references to non-existent columns
    ‚îÇ   ‚îÇ   ‚îî‚îÄ Fix type mismatches in TypeScript
    ‚îÇ   ‚îú‚îÄ Strategy 2 (if code is correct): Generate migration
    ‚îÇ   ‚îÇ   ‚îú‚îÄ Add missing columns to schema
    ‚îÇ   ‚îÇ   ‚îú‚îÄ Create missing tables
    ‚îÇ   ‚îÇ   ‚îî‚îÄ Generate rollback SQL
    ‚îÇ   ‚îî‚îÄ Human Decision Trigger: ambiguous cases (could be code OR schema)
    ‚îÇ
    ‚îú‚îÄ Phase 6: Migration Generation (existing, enhanced)
    ‚îÇ   ‚îî‚îÄ Generate provider-specific migration files
    ‚îÇ
    ‚îú‚îÄ Phase 7: Auto-Push ‚Üê NEW
    ‚îÇ   ‚îú‚îÄ Detect push method for this provider/ORM
    ‚îÇ   ‚îú‚îÄ Push to local first (if available)
    ‚îÇ   ‚îú‚îÄ Push to remote (with confirmation for production)
    ‚îÇ   ‚îî‚îÄ Verify push succeeded
    ‚îÇ
    ‚îú‚îÄ Phase 8: Validation & Type Generation
    ‚îÇ   ‚îú‚îÄ Verify migrations applied correctly
    ‚îÇ   ‚îú‚îÄ Regenerate TypeScript types
    ‚îÇ   ‚îî‚îÄ Verify application builds
    ‚îÇ
    ‚îî‚îÄ Phase 9: Report & SITREP
        ‚îú‚îÄ Full report with drift findings, fixes, migrations
        ‚îî‚îÄ Cleanup
```

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
-->

---

## CONTEXT MANAGEMENT

> Reference: [Context Management Protocol](~/.claude/standards/CONTEXT_MANAGEMENT.md)

### DB-Specific Context Rules

- **Schema snapshots** are written to `.db-reports/schema-snapshot.json` ‚Äî never hold full schema in context
- **Drift scan results** are written to `.db-reports/drift-YYYYMMDD-HHMMSS.json` ‚Äî summarize in context, details on disk
- Sub-agents return: `{ tablesScanned, driftFindings, fixesApplied, errorsEncountered }` (< 500 tokens)
- Full query scan results go to disk, not context

---

## MODES

```
/db                         - Full database audit + drift detection + recommendations
/db drift                   - Schema drift detection only (code vs database)
/db drift --fix             - Detect drift AND auto-fix (code or schema)
/db drift --fix --push      - Detect, fix, AND push to database
/db migrate <name>          - Generate migration for described change
/db diff                    - Compare local schema to remote
/db rollback <migration>    - Generate rollback for a migration
/db seed                    - Generate/update seed data
/db audit                   - Audit RLS, indexes, integrity (no drift)
/db fix DB-XXX              - Fix specific finding
/db types                   - Regenerate TypeScript types
/db push                    - Push pending migrations to database
/db push --production       - Push to production (requires confirmation)
/db assess                  - Show database stack detection + health summary (read-only)
```

---

## CRITICAL RULES

1. **Every migration has a rollback** ‚Äî No exceptions
2. **Test locally first** ‚Äî Verify migration works before recommending for production
3. **Preserve data** ‚Äî Never generate migrations that could lose data without explicit warning
4. **Document changes** ‚Äî Every migration includes comments explaining WHY
5. **Track everything** ‚Äî Write to report file, mark tasks DONE
6. **Validate after** ‚Äî Check integrity after migrations
7. **SITREP conclusion** ‚Äî Historical perspective for audit trail
8. **Prefer code fix over schema change** ‚Äî If drift is detected, fix the code first unless the code is clearly correct and schema is wrong
9. **Database-agnostic** ‚Äî Detect provider and ORM; never assume Supabase
10. **Auto-push requires local success first** ‚Äî Never push to remote without local validation
11. **Production push requires human confirmation** ‚Äî Always pause before pushing to production
12. **Type generation after every schema change** ‚Äî TypeScript types must stay in sync
13. **Never store credentials** ‚Äî Connection strings stay in env vars, never in reports or state files

---

## STATUS UPDATES

This skill follows the **[Status Update Protocol](~/.claude/standards/STATUS_UPDATES.md)**.

### Database Operations Status Flow

```markdown
üöÄ Database Operation Started
   Mode: [audit/drift/migrate/rollback/seed/push]
   Project: [name]
   Provider: [Supabase/Neon/Vercel Postgres/PlanetScale/Turso/Railway/Generic Postgres]
   ORM: [Supabase-JS/Prisma/Drizzle/Kysely/Raw SQL]

üîç Phase 1: Database Stack Detection
   ‚îú‚îÄ Provider: [detected]
   ‚îú‚îÄ ORM: [detected]
   ‚îú‚îÄ Push method: [detected]
   ‚îî‚îÄ ‚úÖ Stack identified

üì∏ Phase 2: Schema Introspection
   ‚îú‚îÄ Tables: [X]
   ‚îú‚îÄ Columns: [X]
   ‚îú‚îÄ Enums: [X]
   ‚îú‚îÄ Functions: [X]
   ‚îî‚îÄ ‚úÖ Schema captured

üîé Phase 3: Schema Drift Detection
   ‚îú‚îÄ Scanning [X] source files for queries
   ‚îú‚îÄ Tables referenced in code: [X]
   ‚îú‚îÄ Columns referenced in code: [X]
   ‚îú‚îÄ ‚ö†Ô∏è Drift findings: [X]
   ‚îÇ   ‚îú‚îÄ Missing columns: [X]
   ‚îÇ   ‚îú‚îÄ Wrong types: [X]
   ‚îÇ   ‚îú‚îÄ Extra columns (unused): [X]
   ‚îÇ   ‚îî‚îÄ Possible renames: [X]
   ‚îî‚îÄ ‚úÖ Drift scan complete

üìä Phase 4: Database Health Audit
   ‚îú‚îÄ Schema Issues: [X]
   ‚îú‚îÄ RLS Issues: [X] (Supabase/Postgres only)
   ‚îú‚îÄ Index Issues: [X]
   ‚îú‚îÄ Integrity Issues: [X]
   ‚îî‚îÄ ‚úÖ Audit complete

üîß Phase 5: Auto-Fix Drift
   ‚îú‚îÄ Code fixes: [X] files updated
   ‚îú‚îÄ Schema fixes: [X] migrations generated
   ‚îú‚îÄ Deferred: [X] (needs human decision)
   ‚îî‚îÄ ‚úÖ Fixes applied

üìù Phase 6: Migration Generation
   ‚îú‚îÄ [X] migrations generated
   ‚îú‚îÄ [X] rollbacks generated
   ‚îî‚îÄ ‚úÖ Migrations ready

üöÄ Phase 7: Auto-Push
   ‚îú‚îÄ Local push: [‚úÖ/‚è≥/‚ùå]
   ‚îú‚îÄ Remote push: [‚úÖ/‚è≥/‚ùå/SKIPPED]
   ‚îî‚îÄ ‚úÖ Push complete

üß™ Phase 8: Validation
   ‚îú‚îÄ Schema verification: [‚úÖ/‚ùå]
   ‚îú‚îÄ Types regenerated: [‚úÖ/‚ùå]
   ‚îú‚îÄ Build check: [‚úÖ/‚ùå]
   ‚îî‚îÄ ‚úÖ Validation passed

üìù Phase 9: Report & SITREP
   ‚îî‚îÄ ‚úÖ Report: .db-reports/db-YYYYMMDD-HHMMSS.md

‚úÖ Database Operation Complete
   Duration: [X] minutes
   Drift findings: [X] found, [X] fixed, [X] deferred
   Migrations: [X] generated, [X] pushed
   Status: [CLEAN/NEEDS_REVIEW/NEEDS_PUSH]
```

---

## AGENT ORCHESTRATION

> Reference: [Agent Orchestration Protocol](~/.claude/standards/AGENT_ORCHESTRATION.md)

### Model Selection

| Task | Model | Why |
|------|-------|-----|
| Database detection + config scanning | haiku | Fast pattern matching |
| Schema introspection | sonnet | Moderate reasoning for SQL parsing |
| Drift detection (code scanning) | sonnet | Pattern matching across codebase |
| Root cause analysis (ambiguous drift) | opus | Deep reasoning for fix-vs-migrate decision |
| Migration generation | sonnet | SQL generation with safety checks |
| Code fix application | sonnet | Code modification with type awareness |
| Validation | haiku | Fast build/test verification |

### Agent Limits

- Max 2 parallel scout agents (schema scan + code scan can run in parallel)
- Fix agents run sequentially (each fix may affect the next)
- Sub-agents return < 500 tokens; full output written to disk
- Resume protocol: check for incomplete state from last 2 hours

---

## REPORT PERSISTENCE

### Finding Lifecycle

```
FOUND ‚Üí FIXING ‚Üí FIXED      (auto-fixed successfully)
FOUND ‚Üí FIXING ‚Üí DEFERRED   (needs human decision)
FOUND ‚Üí FIXING ‚Üí BLOCKED    (cannot fix, dependency issue)
FOUND ‚Üí FIXING ‚Üí REVERTED   (fix broke build, rolled back)
```

### Finding ID Format

| Category | ID Pattern | Example |
|----------|-----------|---------|
| Schema Drift | `DB-DRIFT-NNN` | `DB-DRIFT-001: Column 'workspace_id' in code but not in DB` |
| Schema Health | `DB-SCHEMA-NNN` | `DB-SCHEMA-001: Missing primary key on 'logs' table` |
| RLS | `DB-RLS-NNN` | `DB-RLS-001: RLS disabled on 'users' table` |
| Index | `DB-IDX-NNN` | `DB-IDX-001: Unindexed foreign key on 'posts.author_id'` |
| Integrity | `DB-INT-NNN` | `DB-INT-001: 23 orphaned comment records` |
| Function | `DB-FUNC-NNN` | `DB-FUNC-001: Missing search_path on function` |
| Performance | `DB-PERF-NNN` | `DB-PERF-001: Sequential scan on large table` |
| Push | `DB-PUSH-NNN` | `DB-PUSH-001: Migration failed to apply` |

### State File

```json
{
  "skill": "db",
  "version": "2.0",
  "timestamp": "2026-02-24T15:00:00Z",
  "status": "in_progress",
  "provider": "neon",
  "orm": "drizzle",
  "pushMethod": "drizzle-kit push",
  "stagesCompleted": [0, 1, 2],
  "findings": {
    "DB-DRIFT-001": { "status": "FOUND", "description": "...", "file": "...", "line": 42 },
    "DB-DRIFT-002": { "status": "FIXED", "description": "...", "fixedBy": "code-update" }
  },
  "resources": {},
  "schemaSnapshot": ".db-reports/schema-snapshot.json",
  "driftReport": ".db-reports/drift-20260224-150000.json"
}
```

---

## HUMAN DECISION TRIGGERS

Pause and ask the user when:

1. **Ambiguous drift** ‚Äî Column exists in code AND a similar column exists in DB (possible rename)
2. **Data loss risk** ‚Äî Migration would drop a column with data
3. **Production push** ‚Äî Always confirm before pushing to production
4. **RLS policy removal** ‚Äî Removing security policies requires explicit approval
5. **Table drop** ‚Äî Never drop a table without confirmation
6. **Multiple fix strategies** ‚Äî When code fix AND schema fix are both valid
7. **Type change with data** ‚Äî Changing column type on table with existing data

---

## OUTPUT STRUCTURE

```
.db-reports/
‚îú‚îÄ‚îÄ db-YYYYMMDD-HHMMSS.md              # Main report with task list & conclusion
‚îú‚îÄ‚îÄ drift-YYYYMMDD-HHMMSS.json         # Drift detection results (machine-readable)
‚îú‚îÄ‚îÄ schema-snapshot.json                # Current schema state
‚îú‚îÄ‚îÄ state-YYYYMMDD-HHMMSS.json         # Skill state for resume
‚îú‚îÄ‚îÄ history.json                        # All operations over time
‚îî‚îÄ‚îÄ migrations/
    ‚îî‚îÄ‚îÄ staged/                         # Migrations ready to apply
        ‚îú‚îÄ‚îÄ 20260205_add_workspaces.sql
        ‚îî‚îÄ‚îÄ 20260205_add_workspaces_rollback.sql
```

Also generates to ORM-specific locations:
```
supabase/migrations/              # If Supabase detected
prisma/migrations/                # If Prisma detected
drizzle/                          # If Drizzle detected (drizzle-kit manages)
```

---

## PHASE 1: DATABASE STACK DETECTION

### 1.1 Provider Detection Matrix

Scan in priority order (most specific first):

| Signal | Provider | Confidence |
|--------|----------|-----------|
| `supabase/` directory OR `supabase` in package.json | **Supabase** | HIGH |
| `@neondatabase/serverless` in package.json | **Neon** | HIGH |
| `NEON_DATABASE_URL` or `NEON_` prefixed env vars | **Neon** | HIGH |
| `@vercel/postgres` in package.json | **Vercel Postgres** | HIGH |
| `POSTGRES_URL` env var (Vercel convention) | **Vercel Postgres** | MEDIUM |
| `@planetscale/database` in package.json | **PlanetScale** | HIGH |
| `@libsql/client` or `@turso/` in package.json | **Turso** | HIGH |
| `TURSO_DATABASE_URL` env var | **Turso** | HIGH |
| `DATABASE_URL` with `railway.app` | **Railway** | HIGH |
| `mongodb` or `mongoose` in package.json | **MongoDB** | HIGH |
| `MONGODB_URI` or `MONGO_` env vars | **MongoDB** | HIGH |
| `DATABASE_URL` with `postgres` protocol | **Generic Postgres** | MEDIUM |
| `DATABASE_URL` with `mysql` protocol | **Generic MySQL** | MEDIUM |
| `better-sqlite3` or `sqlite3` in package.json | **SQLite** | HIGH |

**Detection script:**

```bash
# Check config files
[ -d "supabase" ] && echo "PROVIDER:supabase"
[ -f "prisma/schema.prisma" ] && echo "ORM:prisma"
[ -f "drizzle.config.ts" ] || [ -f "drizzle.config.js" ] && echo "ORM:drizzle"

# Check package.json dependencies
DEPS=$(cat package.json 2>/dev/null)
echo "$DEPS" | grep -q "@supabase/supabase-js" && echo "CLIENT:supabase-js"
echo "$DEPS" | grep -q "@prisma/client" && echo "ORM:prisma"
echo "$DEPS" | grep -q "drizzle-orm" && echo "ORM:drizzle"
echo "$DEPS" | grep -q "@neondatabase" && echo "PROVIDER:neon"
echo "$DEPS" | grep -q "@vercel/postgres" && echo "PROVIDER:vercel-postgres"
echo "$DEPS" | grep -q "@planetscale" && echo "PROVIDER:planetscale"
echo "$DEPS" | grep -q "@libsql/client" && echo "PROVIDER:turso"
echo "$DEPS" | grep -q "mongoose" && echo "PROVIDER:mongodb"
echo "$DEPS" | grep -q "kysely" && echo "ORM:kysely"

# Check env files for connection strings
for envfile in .env .env.local .env.development .env.example; do
  if [ -f "$envfile" ]; then
    grep -qE "^SUPABASE_URL=" "$envfile" && echo "PROVIDER:supabase"
    grep -qE "^(NEON_)?DATABASE_URL=.*neon" "$envfile" && echo "PROVIDER:neon"
    grep -qE "^POSTGRES_URL=" "$envfile" && echo "PROVIDER:vercel-postgres"
    grep -qE "^TURSO_DATABASE_URL=" "$envfile" && echo "PROVIDER:turso"
    grep -qE "^MONGODB_URI=" "$envfile" && echo "PROVIDER:mongodb"
  fi
done
```

### 1.2 ORM Detection Matrix

| Signal | ORM | Schema Location | Push Command |
|--------|-----|----------------|-------------|
| `prisma/schema.prisma` exists | **Prisma** | `prisma/schema.prisma` | `npx prisma db push` |
| `drizzle.config.ts` exists | **Drizzle** | Read from config `schema` field | `npx drizzle-kit push` |
| `@supabase/supabase-js` only (no ORM) | **Supabase-JS** | `supabase/migrations/` | `supabase db push` |
| `kysely` in deps | **Kysely** | Custom migration files | Manual |
| `knex` in deps | **Knex** | `knexfile.js` migrations | `npx knex migrate:latest` |
| `typeorm` in deps | **TypeORM** | Entity files | `npx typeorm migration:run` |
| None detected | **Raw SQL** | Manual SQL files | Manual |

### 1.3 Push Method Resolution

Based on detected ORM + Provider:

| ORM | Provider | Push Command (Local) | Push Command (Remote) |
|-----|----------|---------------------|----------------------|
| Supabase-JS | Supabase | `supabase db push --local` | `supabase db push` |
| Prisma | Any Postgres | `npx prisma db push` | `npx prisma db push` (uses DATABASE_URL) |
| Prisma | PlanetScale | `npx prisma db push` | `npx prisma db push` |
| Drizzle | Neon | `npx drizzle-kit push` | `npx drizzle-kit push` (uses DATABASE_URL) |
| Drizzle | Supabase | `npx drizzle-kit push` | `npx drizzle-kit push` |
| Drizzle | Turso | `npx drizzle-kit push` | `npx drizzle-kit push` |
| Drizzle | Vercel Postgres | `npx drizzle-kit push` | `npx drizzle-kit push` |
| Knex | Any | `npx knex migrate:latest` | `npx knex migrate:latest` |
| TypeORM | Any | `npx typeorm migration:run` | `npx typeorm migration:run` |
| Raw SQL | Supabase | `supabase db push --local` | `supabase db push` |
| Raw SQL | Generic Postgres | `psql $DATABASE_URL -f migration.sql` | Same |

### 1.4 Detect Drizzle Schema Location

```typescript
// Read drizzle.config.ts to find schema path
import { readFileSync } from 'fs';

// Parse the schema field from drizzle.config.ts
// schema: "./src/lib/db/schema.ts"  ‚Üí scan this file
// schema: "./src/db/schema/*"       ‚Üí scan this directory
```

### 1.5 Detect Prisma Schema

```bash
# Prisma schema is always at prisma/schema.prisma (or custom path in package.json)
PRISMA_SCHEMA="prisma/schema.prisma"
if [ -f "package.json" ]; then
  CUSTOM_PATH=$(jq -r '.prisma.schema // empty' package.json)
  [ -n "$CUSTOM_PATH" ] && PRISMA_SCHEMA="$CUSTOM_PATH"
fi
```

### 1.6 Stack Summary Output

After detection, emit:

```markdown
## Database Stack Detected

| Component | Detected | Confidence |
|-----------|----------|-----------|
| Provider | Neon (PostgreSQL) | HIGH ‚Äî @neondatabase/serverless in deps |
| ORM | Drizzle | HIGH ‚Äî drizzle.config.ts found |
| Schema | src/lib/db/schema.ts | From drizzle.config.ts |
| Push Method | `npx drizzle-kit push` | Standard for Drizzle |
| Connection | DATABASE_URL env var | .env.local |
| Dialect | PostgreSQL | From drizzle.config.ts dialect field |
```

---

## PHASE 2: SCHEMA INTROSPECTION

### 2.1 Connect and Capture Schema

**Method depends on detected stack:**

#### Supabase (via CLI)
```bash
# List all tables and columns
supabase db dump --schema public --data-only=false

# Or via SQL introspection
SELECT table_name, column_name, data_type, is_nullable, column_default
FROM information_schema.columns
WHERE table_schema = 'public'
ORDER BY table_name, ordinal_position;
```

#### Prisma (via introspection)
```bash
# Pull current schema from database
npx prisma db pull

# The schema.prisma file now reflects actual database state
# Parse it to extract tables, columns, types
```

#### Drizzle (via schema file + database)
```bash
# Drizzle schema is the source of truth for intended schema
# Read the schema.ts file to get intended columns/types
# Then compare against actual database:
npx drizzle-kit introspect

# Or via SQL introspection against DATABASE_URL
```

#### Generic Postgres (via SQL)
```sql
-- Works for Neon, Vercel Postgres, Railway, Supabase, any Postgres
SELECT
  t.table_name,
  c.column_name,
  c.data_type,
  c.udt_name,
  c.is_nullable,
  c.column_default,
  c.character_maximum_length
FROM information_schema.tables t
JOIN information_schema.columns c
  ON t.table_name = c.table_name AND t.table_schema = c.table_schema
WHERE t.table_schema = 'public'
  AND t.table_type = 'BASE TABLE'
ORDER BY t.table_name, c.ordinal_position;

-- Get constraints
SELECT
  tc.table_name,
  tc.constraint_name,
  tc.constraint_type,
  kcu.column_name,
  ccu.table_name AS foreign_table,
  ccu.column_name AS foreign_column
FROM information_schema.table_constraints tc
JOIN information_schema.key_column_usage kcu
  ON tc.constraint_name = kcu.constraint_name
LEFT JOIN information_schema.constraint_column_usage ccu
  ON tc.constraint_name = ccu.constraint_name
WHERE tc.table_schema = 'public';

-- Get indexes
SELECT
  tablename,
  indexname,
  indexdef
FROM pg_indexes
WHERE schemaname = 'public';

-- Get enums
SELECT
  t.typname AS enum_name,
  e.enumlabel AS enum_value
FROM pg_type t
JOIN pg_enum e ON t.oid = e.enumtypid
ORDER BY t.typname, e.enumsortorder;
```

#### PlanetScale / MySQL
```sql
SELECT
  TABLE_NAME,
  COLUMN_NAME,
  DATA_TYPE,
  IS_NULLABLE,
  COLUMN_DEFAULT,
  COLUMN_KEY,
  EXTRA
FROM INFORMATION_SCHEMA.COLUMNS
WHERE TABLE_SCHEMA = DATABASE()
ORDER BY TABLE_NAME, ORDINAL_POSITION;
```

#### Turso / SQLite
```sql
-- List tables
SELECT name FROM sqlite_master WHERE type='table' AND name NOT LIKE 'sqlite_%';

-- For each table, get columns
PRAGMA table_info(table_name);
```

#### MongoDB
```javascript
// List collections
db.getCollectionNames()

// For each collection, sample documents to infer schema
db.collection.aggregate([{ $sample: { size: 100 } }])
// Infer field names, types, and nullability from sample
```

### 2.2 Schema Snapshot Format

```json
{
  "timestamp": "2026-02-24T15:00:00Z",
  "provider": "neon",
  "orm": "drizzle",
  "dialect": "postgresql",
  "tables": {
    "users": {
      "columns": [
        { "name": "id", "type": "text", "nullable": false, "default": null, "primaryKey": true },
        { "name": "email", "type": "text", "nullable": false, "unique": true },
        { "name": "name", "type": "text", "nullable": true },
        { "name": "workspace_id", "type": "uuid", "nullable": true, "references": "workspaces.id" },
        { "name": "subscription_status", "type": "subscription_status", "nullable": false, "default": "'free'" },
        { "name": "created_at", "type": "timestamptz", "nullable": false, "default": "now()" },
        { "name": "updated_at", "type": "timestamptz", "nullable": false, "default": "now()" }
      ],
      "primaryKey": ["id"],
      "foreignKeys": [
        { "column": "workspace_id", "references": "workspaces(id)", "onDelete": "SET NULL" }
      ],
      "indexes": ["users_pkey", "users_email_idx"],
      "rls_enabled": false
    }
  },
  "enums": {
    "subscription_status": ["free", "pro", "enterprise"],
    "platform": ["reddit", "hackernews", "producthunt"]
  },
  "functions": [],
  "triggers": []
}
```

---

## PHASE 3: SCHEMA DRIFT DETECTION

**This is the core new capability.** Scan the codebase for all database queries and compare referenced tables/columns against the actual schema.

### 3.1 Query Pattern Scanning

Scan all source files (`.ts`, `.tsx`, `.js`, `.jsx`) for database query patterns. Each ORM has distinct patterns:

#### Supabase-JS Patterns

```typescript
// Pattern: .from('TABLE').select('COL1, COL2, COL3')
// Extract: table = TABLE, columns = [COL1, COL2, COL3]
supabase.from('users').select('id, email, name, workspace_id')

// Pattern: .from('TABLE').insert({ COL1: val, COL2: val })
// Extract: table = TABLE, columns = [COL1, COL2]
supabase.from('users').insert({ email: 'test@test.com', name: 'Test' })

// Pattern: .from('TABLE').update({ COL: val }).eq('COL', val)
// Extract: table = TABLE, columns from update object + filter columns
supabase.from('users').update({ name: 'New Name' }).eq('id', userId)

// Pattern: .from('TABLE').select('*, RELATION(COL1, COL2)')
// Extract: table = TABLE, all columns + join relation
supabase.from('posts').select('*, author:users(id, name)')

// Pattern: .rpc('FUNCTION_NAME', { PARAM: val })
// Extract: function = FUNCTION_NAME, params = [PARAM]
supabase.rpc('get_user_stats', { user_id: userId })
```

**Regex patterns for Supabase-JS:**
```
\.from\(['"](\w+)['"]\)          ‚Üí table name
\.select\(['"]([^'"]+)['"]\)     ‚Üí column list (comma-separated)
\.insert\(\{([^}]+)\}\)          ‚Üí column names from object keys
\.update\(\{([^}]+)\}\)          ‚Üí column names from object keys
\.eq\(['"](\w+)['"]              ‚Üí filter column
\.order\(['"](\w+)['"]           ‚Üí sort column
\.rpc\(['"](\w+)['"]             ‚Üí function name
```

#### Drizzle ORM Patterns

```typescript
// Pattern: db.query.TABLE.findFirst({ where: eq(TABLE.COL, val) })
// Extract: table = TABLE, columns from where + select
db.query.users.findFirst({
  where: and(eq(users.id, userId), eq(users.workspaceId, wsId)),
})

// Pattern: db.insert(TABLE).values({ COL: val })
// Extract: table from TABLE reference, columns from values object
db.insert(users).values({ id: newId, email: email })

// Pattern: db.select().from(TABLE).where(eq(TABLE.COL, val))
// Extract: table = TABLE, columns from select + where
db.select({ id: users.id, email: users.email }).from(users)

// Pattern: db.update(TABLE).set({ COL: val }).where(eq(TABLE.COL, val))
db.update(users).set({ name: newName }).where(eq(users.id, userId))

// Pattern: db.delete(TABLE).where(eq(TABLE.COL, val))
db.delete(sessions).where(eq(sessions.userId, userId))
```

**For Drizzle, also scan the schema definition file:**
```typescript
// The schema.ts defines the intended shape
export const users = pgTable("users", {
  id: text("id").primaryKey(),
  email: text("email").notNull().unique(),
  // ... compare these against actual database
})
```

#### Prisma Patterns

```typescript
// Pattern: prisma.MODEL.findMany({ select: { COL: true }, where: { COL: val } })
// Extract: model = MODEL, columns from select + where
prisma.user.findMany({
  select: { id: true, email: true, name: true },
  where: { workspaceId: wsId },
})

// Pattern: prisma.MODEL.create({ data: { COL: val } })
prisma.user.create({
  data: { email: 'test@test.com', name: 'Test' },
})

// Pattern: prisma.MODEL.update({ where: { COL: val }, data: { COL: val } })
prisma.user.update({
  where: { id: userId },
  data: { name: newName },
})

// Prisma also has schema.prisma as source of truth
// model User {
//   id    String @id @default(uuid())
//   email String @unique
//   ...
// }
```

**For Prisma, also parse `schema.prisma`:**
```
model\s+(\w+)\s*\{([^}]+)\}     ‚Üí model name + field definitions
(\w+)\s+(String|Int|Boolean|DateTime|Float|Decimal|BigInt|Json|Bytes)\b  ‚Üí field name + type
```

#### Raw SQL Patterns

```sql
-- Pattern: SELECT col1, col2 FROM table WHERE col3 = ...
-- Extract: table, columns from SELECT list + WHERE clause
SELECT id, email, name FROM users WHERE workspace_id = $1;

-- Pattern: INSERT INTO table (col1, col2) VALUES (...)
INSERT INTO users (email, name, workspace_id) VALUES ($1, $2, $3);

-- Pattern: UPDATE table SET col1 = ... WHERE col2 = ...
UPDATE users SET name = $1 WHERE id = $2;
```

### 3.2 Drift Detection Algorithm

For each query found in code:

```
1. Extract table name
2. Check: Does table exist in schema snapshot?
   - NO ‚Üí DRIFT: Table referenced in code but not in database
   - YES ‚Üí Continue

3. Extract column names
4. For each column:
   a. Check: Does column exist in the table?
      - NO ‚Üí DRIFT: Column referenced in code but not in database
      - Check for similar column names (Levenshtein distance ‚â§ 2)
        - Found similar ‚Üí DRIFT (possible rename): 'userName' in code, 'user_name' in DB
        - No similar ‚Üí DRIFT (missing column)
   b. Check: Is the column type compatible with how it's used?
      - Storing string in integer column ‚Üí DRIFT (type mismatch)
      - Using .eq() on non-indexed column ‚Üí WARNING (performance, not drift)

5. For Drizzle/Prisma: Compare ORM schema definition against database
   a. Column in ORM schema but not in DB ‚Üí DRIFT (schema ahead of DB, needs push)
   b. Column in DB but not in ORM schema ‚Üí INFO (DB has extra, may be from another service)
   c. Type mismatch between ORM schema and DB ‚Üí DRIFT (type inconsistency)
```

### 3.3 Drift Classification

| Drift Type | ID Pattern | Severity | Auto-Fix Strategy |
|-----------|-----------|----------|------------------|
| Missing table (code refs non-existent table) | `DB-DRIFT-1xx` | CRITICAL | Generate CREATE TABLE migration |
| Missing column (code refs non-existent column) | `DB-DRIFT-2xx` | HIGH | Fix code OR add column (human decides if ambiguous) |
| Type mismatch (code expects different type) | `DB-DRIFT-3xx` | MEDIUM | Fix code type assertion |
| Possible rename (similar column name) | `DB-DRIFT-4xx` | MEDIUM | Human decision required |
| ORM schema ahead of DB (needs push) | `DB-DRIFT-5xx` | HIGH | Auto-push migration |
| DB ahead of ORM schema (extra columns) | `DB-DRIFT-6xx` | LOW/INFO | Update ORM schema or ignore |
| Missing function (code calls non-existent RPC) | `DB-DRIFT-7xx` | HIGH | Generate function or fix code |
| Enum mismatch (code uses value not in enum) | `DB-DRIFT-8xx` | HIGH | Add enum value or fix code |

### 3.4 Drift Report Format

```json
{
  "timestamp": "2026-02-24T15:00:00Z",
  "provider": "neon",
  "orm": "drizzle",
  "filesScanned": 47,
  "tablesReferenced": 8,
  "columnsReferenced": 42,
  "findings": [
    {
      "id": "DB-DRIFT-201",
      "type": "missing_column",
      "severity": "HIGH",
      "table": "users",
      "column": "avatar_url",
      "referencedIn": [
        { "file": "src/app/api/users/route.ts", "line": 23, "query": ".select('id, email, avatar_url')" },
        { "file": "src/components/user-avatar.tsx", "line": 11, "query": "user.avatar_url" }
      ],
      "existsInDB": false,
      "existsInORMSchema": false,
      "similarColumns": [],
      "recommendation": "REMOVE_FROM_CODE",
      "status": "FOUND"
    },
    {
      "id": "DB-DRIFT-501",
      "type": "orm_schema_ahead",
      "severity": "HIGH",
      "table": "workspaces",
      "column": "billing_email",
      "referencedIn": [
        { "file": "src/lib/db/schema.ts", "line": 45, "definition": "billingEmail: text('billing_email')" }
      ],
      "existsInDB": false,
      "existsInORMSchema": true,
      "recommendation": "PUSH_TO_DB",
      "status": "FOUND"
    }
  ]
}
```

---

## PHASE 4: COMPREHENSIVE AUDIT

### 4.1 Schema Health Checks

| Check | What to Find | Finding ID | Applies To |
|-------|--------------|------------|-----------|
| **Missing Primary Key** | Tables without PK | DB-SCHEMA-001 | All |
| **Missing Timestamps** | No created_at/updated_at | DB-SCHEMA-002 | All |
| **Inconsistent Naming** | Mixed snake_case/camelCase | DB-SCHEMA-003 | All |
| **Missing NOT NULL** | Columns that should be required | DB-SCHEMA-004 | All |
| **No Default Values** | Missing sensible defaults | DB-SCHEMA-005 | All |
| **Wide Tables** | Tables with 20+ columns | DB-SCHEMA-006 | All |
| **Missing Foreign Keys** | Implicit relationships not enforced | DB-SCHEMA-007 | All |
| **Circular Dependencies** | FK cycles that complicate deletes | DB-SCHEMA-008 | SQL databases |

### 4.2 RLS Policy Audit (Supabase/Postgres Only)

| Check | What to Find | Finding ID |
|-------|--------------|------------|
| **RLS Disabled** | Tables without RLS | DB-RLS-001 |
| **No Policies** | RLS enabled but no policies | DB-RLS-002 |
| **Overly Permissive** | `USING (true)` or `WITH CHECK (true)` | DB-RLS-003 |
| **Missing Ownership Filter** | No user_id check | DB-RLS-004 |
| **Subquery Performance** | `auth.uid()` vs `(SELECT auth.uid())` | DB-RLS-005 |
| **Missing INSERT Policy** | Can SELECT but not INSERT | DB-RLS-006 |
| **Missing DELETE Policy** | Can UPDATE but not DELETE | DB-RLS-007 |
| **Admin Bypass Missing** | No service role bypass | DB-RLS-008 |
| **Policy Conflicts** | Overlapping/contradicting policies | DB-RLS-009 |

### 4.3 Index Audit

| Check | What to Find | Finding ID |
|-------|--------------|------------|
| **Unindexed Foreign Keys** | FK columns without index | DB-IDX-001 |
| **Missing Query Indexes** | Columns in WHERE without index | DB-IDX-002 |
| **Missing Sort Indexes** | Columns in ORDER BY without index | DB-IDX-003 |
| **Duplicate Indexes** | Redundant indexes | DB-IDX-004 |
| **Unused Indexes** | Indexes never used | DB-IDX-005 |
| **Missing Composite Index** | Multi-column queries need composite | DB-IDX-006 |
| **Missing Partial Index** | Filtered queries need partial | DB-IDX-007 |
| **Bloated Indexes** | Indexes need REINDEX | DB-IDX-008 |

### 4.4 Data Integrity Audit

| Check | What to Find | Finding ID |
|-------|--------------|------------|
| **Orphaned Records** | FK references to deleted rows | DB-INT-001 |
| **Duplicate Data** | Rows that should be unique | DB-INT-002 |
| **NULL in Required Fields** | NULLs where shouldn't be | DB-INT-003 |
| **Invalid References** | FKs pointing to non-existent rows | DB-INT-004 |
| **Inconsistent Data** | Same entity with different values | DB-INT-005 |
| **Missing Cascade** | ON DELETE should cascade | DB-INT-006 |
| **Stale Data** | Old records that should be archived | DB-INT-007 |

### 4.5 Function & Trigger Audit (Postgres Only)

| Check | What to Find | Finding ID |
|-------|--------------|------------|
| **Missing search_path** | Functions without search_path | DB-FUNC-001 |
| **SECURITY DEFINER Risk** | Elevated privileges without need | DB-FUNC-002 |
| **No EXECUTE Grant** | Functions not callable | DB-FUNC-003 |
| **Trigger Without Function** | Orphaned triggers | DB-FUNC-004 |
| **Missing updated_at Trigger** | No auto-update timestamp | DB-FUNC-005 |

### 4.6 Performance Audit

| Check | What to Find | Finding ID |
|-------|--------------|------------|
| **Large Tables No Partitioning** | 1M+ rows without partitioning | DB-PERF-001 |
| **Missing VACUUM** | Tables need maintenance | DB-PERF-002 |
| **Slow Queries** | Queries taking >1s | DB-PERF-003 |
| **Sequential Scans** | Full table scans on large tables | DB-PERF-004 |
| **Connection Pooling** | No pgBouncer/connection limit | DB-PERF-005 |

---

## PHASE 5: AUTO-FIX DRIFT

### 5.1 Fix Strategy Decision Tree

```
For each drift finding:
‚îÇ
‚îú‚îÄ Missing column in DB, EXISTS in ORM schema
‚îÇ   ‚îî‚îÄ Strategy: PUSH ORM SCHEMA to DB (migration)
‚îÇ      This means the developer added a column to the ORM but hasn't pushed yet
‚îÇ
‚îú‚îÄ Missing column in DB, NOT in ORM schema, referenced in code
‚îÇ   ‚îî‚îÄ Strategy: FIX CODE (remove reference)
‚îÇ      The column doesn't exist anywhere ‚Äî code is referencing something that was never created
‚îÇ      UNLESS: the code is clearly a new feature being built ‚Üí ASK HUMAN
‚îÇ
‚îú‚îÄ Missing column in DB, NOT in ORM schema, similar column exists
‚îÇ   ‚îî‚îÄ Strategy: ASK HUMAN (possible rename)
‚îÇ      Code says 'userName', DB has 'user_name' ‚Äî could be naming convention mismatch
‚îÇ
‚îú‚îÄ Column in DB, missing from ORM schema
‚îÇ   ‚îî‚îÄ Strategy: UPDATE ORM SCHEMA (introspect)
‚îÇ      DB was modified directly or by another service ‚Äî sync ORM to match
‚îÇ      For Prisma: `npx prisma db pull`
‚îÇ      For Drizzle: Add column to schema.ts
‚îÇ
‚îú‚îÄ Type mismatch between code usage and DB
‚îÇ   ‚îî‚îÄ Strategy: FIX CODE (update type assertion/cast)
‚îÇ      Code treats column as string but DB says integer
‚îÇ
‚îú‚îÄ Enum value in code but not in DB enum
‚îÇ   ‚îî‚îÄ Strategy: ADD ENUM VALUE (migration)
‚îÇ      New feature uses new enum value that needs to be added
‚îÇ
‚îú‚îÄ Table in code but not in DB
‚îÇ   ‚îî‚îÄ Strategy: ASK HUMAN
‚îÇ      Could be new table (needs migration) or dead code (needs removal)
‚îÇ
‚îî‚îÄ Function/RPC in code but not in DB
    ‚îî‚îÄ Strategy: GENERATE FUNCTION (migration)
       Code calls supabase.rpc('func_name') but function doesn't exist
```

### 5.2 Code Fix Patterns

**Fix: Remove reference to non-existent column (Supabase-JS)**
```typescript
// Before (column 'avatar_url' doesn't exist):
const { data } = await supabase.from('users').select('id, email, avatar_url')

// After:
const { data } = await supabase.from('users').select('id, email')
```

**Fix: Update column name to match DB (Drizzle)**
```typescript
// Before (code uses camelCase, DB uses snake_case):
db.query.users.findFirst({ where: eq(users.userName, name) })

// After:
db.query.users.findFirst({ where: eq(users.user_name, name) })

// Better: Fix in Drizzle schema to map correctly:
userName: text("user_name")  // Drizzle handles the mapping
```

**Fix: Update TypeScript type to match DB column type**
```typescript
// Before (DB column is integer, code treats as string):
const count: string = user.login_count

// After:
const count: number = user.login_count
```

### 5.3 Migration Fix Patterns

**Fix: Add missing column to DB (ORM schema is ahead)**

For Drizzle:
```bash
# Schema already has the column definition
# Just push it:
npx drizzle-kit push
```

For Prisma:
```bash
# Schema already has the field
# Just push it:
npx prisma db push
```

For Supabase (raw SQL):
```sql
-- Generate migration:
ALTER TABLE users ADD COLUMN avatar_url TEXT;
```

**Fix: Add missing enum value**
```sql
-- PostgreSQL
ALTER TYPE subscription_status ADD VALUE 'team';

-- Note: Cannot remove enum values in PostgreSQL without recreating the type
```

**Fix: Create missing function**
```sql
CREATE OR REPLACE FUNCTION get_user_stats(p_user_id UUID)
RETURNS JSON
LANGUAGE plpgsql
SECURITY DEFINER
SET search_path = public
AS $$
BEGIN
  -- Generated stub ‚Äî developer must implement logic
  RETURN json_build_object('user_id', p_user_id, 'stub', true);
END;
$$;
```

### 5.4 Fix Verification

After every fix:

1. **Code fix** ‚Üí Run `tsc --noEmit` to verify TypeScript compiles
2. **Schema migration** ‚Üí Apply locally, run `npx prisma validate` or `npx drizzle-kit check`
3. **Push** ‚Üí Verify with schema introspection that column/table now exists
4. **Build check** ‚Üí Run full build to catch any cascading issues
5. **If fix breaks build** ‚Üí REVERT the fix, mark as DEFERRED, log the error

---

## PHASE 6: MIGRATION GENERATION

### 6.1 Migration File Structure

**Filename format:** `YYYYMMDD_HHMMSS_<description>.sql`

**Migration template:**
```sql
-- Migration: <description>
-- Created: YYYY-MM-DD HH:MM:SS
-- Author: /db skill
-- Provider: <detected_provider>
-- ORM: <detected_orm>
--
-- Purpose: <why this migration exists>
--
-- Changes:
--   - <change 1>
--   - <change 2>
--
-- Rollback: YYYYMMDD_HHMMSS_<description>_rollback.sql

-- ============================================================================
-- PRE-MIGRATION CHECKS
-- ============================================================================

DO $$
BEGIN
  -- Check table exists (example)
  IF NOT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_name = 'users') THEN
    RAISE EXCEPTION 'Precondition failed: users table does not exist';
  END IF;
END $$;

-- ============================================================================
-- MIGRATION
-- ============================================================================

-- <actual migration SQL here>

-- ============================================================================
-- POST-MIGRATION VALIDATION
-- ============================================================================

DO $$
BEGIN
  -- Verify migration succeeded
  IF NOT EXISTS (SELECT 1 FROM information_schema.columns
                 WHERE table_name = 'users' AND column_name = 'workspace_id') THEN
    RAISE EXCEPTION 'Migration validation failed: workspace_id column not created';
  END IF;
END $$;
```

**Rollback template:**
```sql
-- Rollback: <description>
-- Reverses: YYYYMMDD_HHMMSS_<description>.sql
-- Created: YYYY-MM-DD HH:MM:SS
--
-- WARNING: This rollback may result in data loss if:
--   - <condition 1>

-- ============================================================================
-- ROLLBACK
-- ============================================================================

-- <rollback SQL here>

-- ============================================================================
-- POST-ROLLBACK VALIDATION
-- ============================================================================

DO $$
BEGIN
  -- Verify rollback succeeded
END $$;
```

### 6.2 ORM-Specific Migration Generation

**For Prisma:** Migrations are generated by Prisma CLI, not raw SQL:
```bash
npx prisma migrate dev --name <description>
# This creates prisma/migrations/<timestamp>_<name>/migration.sql
```

**For Drizzle:** Drizzle-kit generates migrations:
```bash
npx drizzle-kit generate --name <description>
# This creates drizzle/<timestamp>_<name>.sql
```

**For Supabase:** Raw SQL migrations:
```bash
# Create migration file
supabase migration new <description>
# This creates supabase/migrations/<timestamp>_<description>.sql
# Then write the SQL into it
```

### 6.3 Common Migration Patterns

**Add Column (Safe):**
```sql
ALTER TABLE users ADD COLUMN workspace_id UUID REFERENCES workspaces(id);

-- Backfill existing rows
UPDATE users SET workspace_id = (SELECT id FROM workspaces WHERE is_default = true LIMIT 1)
WHERE workspace_id IS NULL;

-- Make required after backfill
ALTER TABLE users ALTER COLUMN workspace_id SET NOT NULL;

-- Add index
CREATE INDEX CONCURRENTLY idx_users_workspace_id ON users(workspace_id);
```

**Create Table:**
```sql
CREATE TABLE workspaces (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  name TEXT NOT NULL,
  slug TEXT NOT NULL UNIQUE,
  owner_id UUID NOT NULL REFERENCES auth.users(id) ON DELETE CASCADE,
  created_at TIMESTAMPTZ NOT NULL DEFAULT now(),
  updated_at TIMESTAMPTZ NOT NULL DEFAULT now()
);

-- RLS (Supabase only)
ALTER TABLE workspaces ENABLE ROW LEVEL SECURITY;

CREATE POLICY "Users can view their workspaces"
  ON workspaces FOR SELECT
  USING (owner_id = auth.uid());
```

**Change Column Type (Careful):**
```sql
-- Validate existing data first
DO $$
DECLARE invalid_count INTEGER;
BEGIN
  SELECT COUNT(*) INTO invalid_count FROM orders
  WHERE status NOT IN ('pending', 'processing', 'shipped', 'delivered', 'cancelled');
  IF invalid_count > 0 THEN
    RAISE EXCEPTION 'Found % rows with invalid status values', invalid_count;
  END IF;
END $$;

ALTER TABLE orders ALTER COLUMN status TYPE order_status USING status::order_status;
```

**Add Index (Non-Blocking):**
```sql
CREATE INDEX CONCURRENTLY idx_posts_author_created ON posts(author_id, created_at DESC);
```

---

## PHASE 7: AUTO-PUSH

### 7.1 Push Decision Matrix

```
Is there anything to push?
‚îú‚îÄ NO ‚Üí Skip (report: "Schema is in sync")
‚îî‚îÄ YES
    ‚îÇ
    ‚îú‚îÄ Has local environment?
    ‚îÇ   ‚îú‚îÄ YES ‚Üí Push locally first ‚Üí Verify ‚Üí Continue to remote
    ‚îÇ   ‚îî‚îÄ NO ‚Üí Push to remote directly (with extra caution)
    ‚îÇ
    ‚îú‚îÄ Is target production?
    ‚îÇ   ‚îú‚îÄ YES ‚Üí HUMAN DECISION REQUIRED
    ‚îÇ   ‚îÇ   "Migration X will alter table Y (Z rows). Proceed?"
    ‚îÇ   ‚îî‚îÄ NO (dev/preview) ‚Üí Auto-push
    ‚îÇ
    ‚îî‚îÄ Push
        ‚îú‚îÄ SUCCESS ‚Üí Log, verify, continue
        ‚îî‚îÄ FAILURE ‚Üí Log error, mark BLOCKED, do NOT retry blindly
```

### 7.2 Push Commands by Stack

**Supabase:**
```bash
# Local (Supabase CLI running)
supabase db push --local

# Remote (linked project)
supabase db push
# Requires: supabase link --project-ref <ref>
```

**Prisma:**
```bash
# Development (direct push, no migration history)
npx prisma db push

# Production (with migration history)
npx prisma migrate deploy
```

**Drizzle:**
```bash
# Push schema to database
npx drizzle-kit push

# Or apply generated migrations
npx drizzle-kit migrate
```

**Knex:**
```bash
npx knex migrate:latest
```

**TypeORM:**
```bash
npx typeorm migration:run -d src/data-source.ts
```

**Raw SQL:**
```bash
# Generic Postgres
psql "$DATABASE_URL" -f path/to/migration.sql
```

### 7.3 Push Verification

After push, verify the schema matches expectations:

```bash
# Re-introspect the database
# Compare new snapshot against expected schema
# All drift findings that were PUSH_TO_DB should now be resolved
```

### 7.4 Push Safety

- **Never push destructive changes without confirmation:** DROP TABLE, DROP COLUMN, ALTER TYPE
- **Always check row counts before destructive operations:**
  ```sql
  SELECT COUNT(*) FROM table_to_alter;
  -- If > 0 rows, warn about data implications
  ```
- **Backup recommendation for production:**
  ```markdown
  ‚ö†Ô∏è Before pushing to production, consider:
  1. Take a database snapshot/backup
  2. Review the migration SQL carefully
  3. Test in a staging environment first
  4. Have a rollback plan ready
  ```

---

## PHASE 8: VALIDATION & TYPE GENERATION

### 8.1 Post-Push Validation

```sql
-- Re-run schema introspection
-- Compare against expected state
-- Verify: all drift findings marked PUSH_TO_DB are now resolved
-- Verify: no new drift introduced
```

### 8.2 Type Generation

**Supabase:**
```bash
supabase gen types typescript --local > src/types/database.ts
# Or from remote:
supabase gen types typescript --project-id $PROJECT_ID > src/types/database.ts
```

**Prisma:**
```bash
npx prisma generate
# Types are auto-generated in node_modules/.prisma/client/
```

**Drizzle:**
```bash
# Drizzle types come from the schema.ts file directly
# No separate type generation needed ‚Äî the schema IS the types
# But verify the schema matches the DB after push:
npx drizzle-kit check
```

### 8.3 Build Verification

```bash
# Detect package manager
PM="npm"
[ -f "pnpm-lock.yaml" ] && PM="pnpm"
[ -f "bun.lockb" ] && PM="bun"
[ -f "yarn.lock" ] && PM="yarn"

# Type check
$PM run typecheck 2>/dev/null || npx tsc --noEmit

# Build (catches runtime import issues)
$PM run build
```

If build fails after type regeneration:
1. Read the errors
2. Fix type mismatches in code
3. Re-verify build
4. If still failing after 3 attempts ‚Üí mark as DEFERRED with error details

---

## PHASE 9: REPORT & SITREP

### 9.1 Main Report File

```markdown
# Database Operations Report: [PROJECT_NAME]

**Created:** [YYYY-MM-DD HH:MM:SS]
**Provider:** [detected]
**ORM:** [detected]
**Status:** üü¢ COMPLETE

---

## Database Stack

| Component | Value |
|-----------|-------|
| Provider | Neon (PostgreSQL) |
| ORM | Drizzle |
| Schema File | src/lib/db/schema.ts |
| Push Method | drizzle-kit push |
| Connection | DATABASE_URL |

---

## Schema Drift Detection

### Summary

| Metric | Count |
|--------|-------|
| Files scanned | 47 |
| Tables referenced in code | 8 |
| Columns referenced in code | 42 |
| Drift findings | 3 |
| Auto-fixed (code) | 1 |
| Auto-fixed (migration) | 1 |
| Deferred (human decision) | 1 |

### Findings

| ID | Type | Table.Column | Severity | Status | Fix |
|----|------|-------------|----------|--------|-----|
| DB-DRIFT-201 | Missing column | users.avatar_url | HIGH | ‚úÖ FIXED | Removed from select query |
| DB-DRIFT-501 | ORM ahead of DB | workspaces.billing_email | HIGH | ‚úÖ FIXED | Pushed to DB |
| DB-DRIFT-401 | Possible rename | users.userName ‚Üí user_name | MEDIUM | ‚è≥ DEFERRED | Human decision needed |

---

## Audit Results

| Category | Findings | Fixed |
|----------|----------|-------|
| Schema Issues | 3 | 3 ‚úÖ |
| RLS Issues | 5 | 5 ‚úÖ |
| Index Issues | 4 | 4 ‚úÖ |
| Integrity Issues | 1 | 1 ‚úÖ |
| Performance Issues | 2 | 2 ‚úÖ |

---

## Migrations

| Migration | Description | Local | Remote |
|-----------|-------------|-------|--------|
| 20260224_150000 | Add billing_email to workspaces | ‚úÖ Applied | ‚úÖ Pushed |

---

## SITREP (Conclusion)

### Mission Status: üü¢ COMPLETE

**Duration:** 4 minutes 18 seconds

### What Was Accomplished

1. **Schema Drift Detected and Fixed**
   - Found 3 drift findings across 47 source files
   - Auto-fixed 2 (1 code fix, 1 migration push)
   - 1 deferred for human review (possible rename)

2. **Database Health Audited**
   - [Summary of audit findings and fixes]

3. **Types Regenerated**
   - Drizzle schema verified against database
   - Application builds successfully

### Pending Actions

1. Review deferred finding DB-DRIFT-401 (possible rename)
2. [Any other pending items]
```

### 9.2 Update History

```json
{
  "operations": [
    {
      "id": 1,
      "timestamp": "2026-02-24T15:00:00Z",
      "type": "full",
      "provider": "neon",
      "orm": "drizzle",
      "duration_seconds": 258,
      "drift": {
        "filesScanned": 47,
        "findings": 3,
        "fixed": 2,
        "deferred": 1
      },
      "audit": {
        "schema": { "found": 3, "fixed": 3 },
        "rls": { "found": 0, "fixed": 0 },
        "index": { "found": 4, "fixed": 4 },
        "integrity": { "found": 1, "fixed": 1 }
      },
      "migrations_generated": 1,
      "migrations_pushed": 1,
      "report_file": ".db-reports/db-20260224-150000.md"
    }
  ]
}
```

---

## SCHEMA DIFF (Existing Capability, Enhanced)

### Compare Local Schema to Remote

**Supabase:**
```bash
supabase db dump --local --schema public > .db-reports/local-schema.sql
supabase db dump --schema public > .db-reports/remote-schema.sql
diff .db-reports/local-schema.sql .db-reports/remote-schema.sql
```

**Prisma:**
```bash
# Prisma's migrate diff command
npx prisma migrate diff --from-schema-datasource prisma/schema.prisma --to-schema-datamodel prisma/schema.prisma
```

**Drizzle:**
```bash
# Drizzle-kit shows pending changes
npx drizzle-kit check
```

### Diff Report Format

```markdown
## Schema Diff: Local vs Remote

### Tables
| Table | Local | Remote | Action Needed |
|-------|-------|--------|---------------|
| workspaces | ‚úÖ Has billing_email | ‚ùå Missing column | ALTER TABLE |
| users | ‚úÖ Same | ‚úÖ Same | None |

### Columns Added Locally
| Table | Column | Type | Migration |
|-------|--------|------|-----------|
| workspaces | billing_email | TEXT | Pending push |
```

---

## SEED DATA MANAGEMENT

### Seed File Structure

**Supabase:**
```
supabase/seed.sql
supabase/seeds/
‚îú‚îÄ‚îÄ 01_users.sql
‚îú‚îÄ‚îÄ 02_workspaces.sql
‚îî‚îÄ‚îÄ 03_test_data.sql
```

**Prisma:**
```
prisma/seed.ts        # TypeScript seed script
```

**Drizzle:**
```
src/lib/db/seed.ts    # Custom seed script using Drizzle ORM
```

### Seed Safety
```sql
-- Always check environment before seeding
DO $$
BEGIN
  IF current_setting('app.environment', true) = 'production' THEN
    RAISE EXCEPTION 'Cannot run seed data in production';
  END IF;
END $$;
```

---

## SAFETY GUIDELINES

### Always Do:
- Generate rollback for every migration
- Test migration locally before recommending for production
- Validate data integrity after changes
- Use CONCURRENTLY for index creation on large tables
- Check for data loss implications
- Detect provider before assuming any specific tool

### Never Do:
- DROP TABLE without explicit user confirmation
- DROP COLUMN with data without warning
- Disable RLS without explicit reason
- Remove indexes without checking query patterns
- Modify production directly (always use migrations)
- Store connection strings in reports or state files
- Assume Supabase ‚Äî always detect first

### Data Loss Warnings

When a migration could lose data, always warn:

```markdown
‚ö†Ô∏è **DATA LOSS WARNING**

This migration will:
- DROP COLUMN `legacy_id` from `users` table
- Affected rows: 45,234

If you need this data:
1. Export before migration: `SELECT id, legacy_id FROM users`
2. Store in backup table: `CREATE TABLE users_legacy AS SELECT ...`

Proceed only if you've backed up or don't need this data.
```

---

## CLEANUP PROTOCOL

> Reference: [Resource Cleanup Protocol](~/.claude/standards/CLEANUP_PROTOCOL.md)

### DB-Specific Cleanup

Resources this skill may create:
- Seed data in local database (test users, sample records)
- Schema dump files in `.db-reports/` directory
- Migration rollback SQL files
- Schema snapshot JSON files
- Drift detection JSON files

Cleanup actions:
1. **Seed data disclosure:** After seeding, note in the report: "Seed data applied to local database. Run `supabase db reset` / `npx prisma migrate reset` to clear"
2. **Schema dump files:** Keep in `.db-reports/` (intended output for schema tracking)
3. **Gitignore enforcement:** Ensure `.db-reports/` is in `.gitignore`
4. **Sensitive data check:** Verify seed files do not contain real user data, production secrets, or PII. Flag if found
5. **Connection string safety:** Verify no DATABASE_URL or credentials leaked into report files

Cleanup verification:
- `.db-reports/` directory is gitignored
- Seed data is documented in the report
- No real credentials in seed files or reports
- No connection strings in any output file

---

## REMEMBER

- **Every migration has a rollback** ‚Äî No exceptions
- **Test locally first** ‚Äî Verify before production
- **Preserve data** ‚Äî Warn about any data loss
- **Document WHY** ‚Äî Future you will thank you
- **Mark DONE, never delete** ‚Äî Audit trail matters
- **SITREP conclusion** ‚Äî Historical perspective
- **Types stay in sync** ‚Äî Regenerate after schema changes
- **Detect first, assume never** ‚Äî Always identify provider + ORM before taking action
- **Code fix before schema change** ‚Äî Prefer fixing code over altering the database
- **Production push = human confirmation** ‚Äî Never auto-push to production

<!-- Claude Code Skill by Steel Motion LLC ‚Äî https://steelmotion.dev -->
<!-- Part of the Claude Code Skills Collection -->
<!-- Powered by Claude models: Haiku (fast extraction), Sonnet (balanced reasoning), Opus (deep analysis) -->
<!-- License: MIT -->
